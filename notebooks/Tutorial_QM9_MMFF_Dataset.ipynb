{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(partition,total_num, random_seed = 1, save = True):\n",
    "    ### total data index ###\n",
    "    data_id_list = [i for i in range(1,total_num)]\n",
    "    ### get partion for each type of data ###\n",
    "    [tran,valn,testln] = [partition[i] for i in [\"train\",\"validation\",\"test_live\"]]\n",
    "    ### whether or not to use random seed\n",
    "    if random_seed:\n",
    "        np.random.seed(random_seed)\n",
    "    np.random.shuffle(data_id_list)\n",
    "    train_id = data_id_list[:tran]\n",
    "    val_id = data_id_list[tran:tran + valn]\n",
    "    test_live_id = data_id_list[tran + valn:tran + valn + testln]\n",
    "    test_id = data_id_list[tran + valn + testln:]\n",
    "    if save:\n",
    "        np.savez(\"split_infor.npz\",train= train_id, test = test_id, validation = val_id, test_live = test_live_id)\n",
    "    \n",
    "    return train_id, val_id, test_live_id, test_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### total number of data is 133885\n",
    "### train: 99000 validation:1000 test_live:1000 test:32885\n",
    "partition = {\"train\":99000,\"validation\":1000,\"test_live\":1000}\n",
    "total_num = 133885\n",
    "train_id,val_id,test_live_id,test_id = split_data(partition, total_num, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate tfrecord file for training, testing, validation and test_live"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import tempfile\n",
    "from tempfile import TemporaryDirectory\n",
    "from ase.units import Hartree, eV, Bohr, Ang\n",
    "import rdkit \n",
    "from rdkit import Chem \n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import tensorflow as tf;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar = tarfile.open(\"qm9_mmff.tar.bz2\")\n",
    "element_conversions = {\"H\":1,\"C\":6,\"O\":8,\"N\":7,\"F\":9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list = tf.train.BytesList(value = value))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list = tf.train.Int64List(value = value))\n",
    "\n",
    "def _float64_feature(value):\n",
    "    return tf.train.Feature(float_list = tf.train.FloatList(value = value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "### use 10 molecules as exmaples ### \n",
    "def write_tfrecord(outfile,data_list):\n",
    "    olddir = os.getcwd()\n",
    "    writer = tf.python_io.TFRecordWriter(outfile)\n",
    "    for data in data_list[0:10]:\n",
    "        ### save into temporary directory which will be deleted automatically after exit ###\n",
    "        with TemporaryDirectory() as temp_dir:\n",
    "            raw_path = os.path.join(temp_dir, 'qm9_mmff_file')\n",
    "            print(raw_path)\n",
    "            tmpsdf =\"./\" + str(data) + \".sdf\"\n",
    "            tmpmmffsdf = \"./\" + str(data) + \".mmff.sdf\"\n",
    "            tar.extract(tmpsdf, path = raw_path)\n",
    "            tar.extract(tmpmmffsdf, path = raw_path)\n",
    "            os.chdir(raw_path)\n",
    "            with open(tmpsdf,\"r\") as f:\n",
    "                lines = f.readlines()\n",
    "            with open(tmpmmffsdf,\"r\") as fm:\n",
    "                linesm = fm.readlines()\n",
    "            targets = [float(i) for i in lines[0].split()]\n",
    "            ### using atom_num to get element number and coordinates ####\n",
    "            atom_num = int(lines[3].split()[0])\n",
    "            print(atom_num)\n",
    "            elements= [element_conversions[line.split()[3]] for line in lines[4:4+atom_num]]      \n",
    "            positions = [line.split()[0:3] for line in lines[4:4+atom_num]]\n",
    "            mmffpositions = [line.split()[0:3] for line in linesm[4:4+atom_num]]\n",
    "            \n",
    "            elements = np.array(elements).astype(np.int64)\n",
    "            targets = np.array(targets).astype(np.float32)\n",
    "            positions = np.array(positions).astype(np.float32)\n",
    "            mmffpositions = np.array(mmffpositions).astype(np.float32)\n",
    "            \n",
    "            newfeatures = {'elements' : _bytes_feature([elements.tostring()]),\n",
    "                           'positions' : _bytes_feature([positions.ravel().tostring()]),\n",
    "                           'mmffpositions' : _bytes_feature([mmffpositions.ravel().tostring()]),\n",
    "                           'targets' : _bytes_feature([targets.tostring()])}\n",
    "            \n",
    "            example = tf.train.Example(features=tf.train.Features(feature=newfeatures))\n",
    "            writer.write(example.SerializeToString())\n",
    "            os.chdir(olddir)\n",
    "    writer.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/var/folders/l2/vbjcs9zx69722gqn21_25kg80000gp/T/tmpobl5a0jk/qm9_mmff_file\n",
      "22\n",
      "/var/folders/l2/vbjcs9zx69722gqn21_25kg80000gp/T/tmp9pzsyzak/qm9_mmff_file\n",
      "15\n",
      "/var/folders/l2/vbjcs9zx69722gqn21_25kg80000gp/T/tmpiwdlru0_/qm9_mmff_file\n",
      "15\n",
      "/var/folders/l2/vbjcs9zx69722gqn21_25kg80000gp/T/tmpzmsjbq3g/qm9_mmff_file\n",
      "13\n",
      "/var/folders/l2/vbjcs9zx69722gqn21_25kg80000gp/T/tmp5_zyxdzz/qm9_mmff_file\n",
      "19\n",
      "/var/folders/l2/vbjcs9zx69722gqn21_25kg80000gp/T/tmp2l72fbps/qm9_mmff_file\n",
      "21\n",
      "/var/folders/l2/vbjcs9zx69722gqn21_25kg80000gp/T/tmpe9pj6t8w/qm9_mmff_file\n",
      "18\n",
      "/var/folders/l2/vbjcs9zx69722gqn21_25kg80000gp/T/tmp4a_smoou/qm9_mmff_file\n",
      "20\n",
      "/var/folders/l2/vbjcs9zx69722gqn21_25kg80000gp/T/tmpo1hpophy/qm9_mmff_file\n",
      "13\n",
      "/var/folders/l2/vbjcs9zx69722gqn21_25kg80000gp/T/tmpyg9uvad_/qm9_mmff_file\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "write_tfrecord(\"test_train.tfrecord\",train_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.5030e-01  1.5706e+00  3.5000e-02]\n",
      " [-2.0000e-02  3.5600e-02  3.3000e-03]\n",
      " [-1.4249e+00 -5.8550e-01  4.2000e-03]\n",
      " [ 7.4270e-01 -4.1970e-01 -1.2611e+00]\n",
      " [ 2.1783e+00  1.2230e-01 -1.3285e+00]\n",
      " [ 2.8857e+00  9.0000e-03 -4.7000e-03]\n",
      " [ 2.2528e+00 -2.4910e-01  1.1392e+00]\n",
      " [ 7.6270e-01 -4.5090e-01  1.2462e+00]\n",
      " [ 8.2200e-01  2.0606e+00  1.3580e-01]\n",
      " [-6.2530e-01  1.9433e+00 -8.7960e-01]\n",
      " [-7.6860e-01  1.8873e+00  8.8230e-01]\n",
      " [-2.0179e+00 -2.2300e-01 -8.4320e-01]\n",
      " [-1.3761e+00 -1.6779e+00 -6.5100e-02]\n",
      " [-1.9680e+00 -3.3250e-01  9.2200e-01]\n",
      " [ 7.7890e-01 -1.5170e+00 -1.2642e+00]\n",
      " [ 1.8550e-01 -1.2350e-01 -2.1583e+00]\n",
      " [ 2.7425e+00 -4.1810e-01 -2.0994e+00]\n",
      " [ 2.1791e+00  1.1722e+00 -1.6569e+00]\n",
      " [ 3.9636e+00  1.5440e-01 -4.6000e-03]\n",
      " [ 2.8226e+00 -3.3030e-01  2.0624e+00]\n",
      " [ 3.8400e-01  6.0800e-02  2.1417e+00]\n",
      " [ 5.5570e-01 -1.5184e+00  1.4167e+00]]\n"
     ]
    }
   ],
   "source": [
    "### Check File ###\n",
    "record = tf.data.TFRecordDataset(\"test_train.tfrecord\")\n",
    "def _parser_function(record):\n",
    "    features = {\"elements\":tf.FixedLenFeature([], tf.string),\n",
    "                \"positions\":tf.FixedLenFeature([], tf.string),\n",
    "                \"mmffpositions\":tf.FixedLenFeature([], tf.string),\n",
    "                \"targets\":tf.FixedLenFeature([], tf.string)}\n",
    "    parsed_features = tf.parse_single_example(record,features)\n",
    "    features_new = {}\n",
    "    dtype = {\"elements\":tf.int64,\"targets\":tf.float32,\"positions\":tf.float32,\"mmffpositions\":tf.float32}\n",
    "    for i in parsed_features.keys():\n",
    "        feat = tf.decode_raw(parsed_features[i],dtype[i])\n",
    "        if i == \"positions\" or i == \"mmffpositions\":\n",
    "            feat = tf.reshape(feat,[-1,3])\n",
    "        features_new[i] = feat\n",
    "    return features_new\n",
    "\n",
    "record = record.map(_parser_function)\n",
    "record = record.repeat()\n",
    "record = record.padded_batch(batch_size = 10, padded_shapes={\"elements\":[None],\"positions\":[None,3],\"mmffpositions\":[None,3],\"targets\":[None],}, padding_values=None)\n",
    "iterator  = record.make_initializable_iterator()\n",
    "features_new = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(iterator.initializer)\n",
    "    features = sess.run(features_new)\n",
    "    print(features[\"positions\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6308, 91703, 101162, 13777, 5246, 86608, 103353, 118248, 96799, 12700]\n"
     ]
    }
   ],
   "source": [
    "print(train_id[0:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
